# LLM configuration stub
CACHED_PROVIDER = "stub"
PROVIDER = "ollama"  # options: 'stub', 'ollama', 'openai'
MODEL = "llama2"  # default local model name for Ollama (override as needed)
OLLAMA_HTTP_URL = "http://localhost:11434"  # default Ollama HTTP endpoint
CACHE_PATH = "velinor/cache/dialogue/"
SAFE_WORDS = ["glyph", "resonance", "ritual"]
DEFAULT_VARIANT = "deterministic"
