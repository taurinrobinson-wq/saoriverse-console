""" Audio Conversation System - Integration Guide & Architecture

This document outlines the complete audio conversation pipeline for FirstPerson, including the
orchestrator, prosody planning, and Streamlit integration.

Table of Contents:

1. Architecture Overview 2. Components & Responsibilities 3. Data Flow 4. Integration with
FirstPerson Pipeline 5. Usage Examples 6. Troubleshooting """

# ============================================================================

# 1. ARCHITECTURE OVERVIEW

# ============================================================================

""" â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     AUDIO CONVERSATION PIPELINE                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RECORD     â”‚ --> â”‚ TRANSCRIBE   â”‚ --> â”‚   PROCESS    â”‚ --> â”‚ PLAN PROSODY â”‚
â”‚   (Audio)    â”‚     â”‚  (Whisper)   â”‚     â”‚(FirstPerson) â”‚     â”‚  (Glyphs)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                ^                      |
                                                |                      v
context:              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â€¢ history               â”‚ SYNTHESIZE   â”‚ â€¢ turn count
â”‚   & STREAM   â”‚ â€¢ settings              â”‚   (TTS)      â”‚
                                                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                      |
v â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                                 â”‚   PLAYBACK   â”‚
                                                                 â”‚(Non-blocking)â”‚
                                                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                      |
v [Loop back or Stop]

KEY FEATURES: â€¢ Glyph Intent â†’ Prosody: voltage/tone/certainty control speech characteristics â€¢
Non-blocking Playback: synthesis overlaps with playback (minimal latency) â€¢ Streaming TTS: text
chunked intelligently at sentence/phrase boundaries â€¢ Auto-silence Detection: recording stops after
1.5s of quiet â€¢ Pause/Resume/Stop: user controls at any time â€¢ State Machine: UI always knows
current conversation state """

# ============================================================================

# 2. COMPONENTS & RESPONSIBILITIES

# ============================================================================

""" â”Œâ”€ AudioRecorder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Captures user speech with automatic silence detection                    â”‚
â”‚ â€¢ Records 16kHz mono audio                                               â”‚
â”‚ â€¢ Auto-stops after 1.5s silence                                          â”‚
â”‚ â€¢ Thread-safe with stop event support                                    â”‚
â”‚ â€¢ Returns: np.ndarray of audio samples                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ ProsodyPlanner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Converts glyph signals to SSML prosody tags                              â”‚
â”‚                                                                            â”‚
â”‚ Glyph Intent Inputs:                                                     â”‚
â”‚ â€¢ voltage: "low" | "medium" | "high"                                    â”‚
â”‚   â†’ Affects: rate (slow/normal/fast), volume (soft/normal/loud)         â”‚
â”‚                                                                            â”‚
â”‚ â€¢ tone: "negative" | "neutral" | "positive"                             â”‚
â”‚   â†’ Affects: pitch (low/medium/high)                                     â”‚
â”‚                                                                            â”‚
â”‚ â€¢ certainty: "low" | "neutral" | "high"                                 â”‚
â”‚   â†’ Affects: intonation contour (rising/neutral/falling)                 â”‚
â”‚                                                                            â”‚
â”‚ â€¢ energy: 0.0-1.0 (float)                                                â”‚
â”‚   â†’ Affects: overall intensity modulation                                â”‚
â”‚                                                                            â”‚
â”‚ â€¢ hesitation: bool                                                        â”‚
â”‚   â†’ Adds strategic pauses for natural speech                             â”‚
â”‚                                                                            â”‚
â”‚ Output: SSML-marked text ready for TTS                                   â”‚
â”‚ Example: "<prosody rate='fast' pitch='high' volume='loud'>Text</prosody>"â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ TextToSpeechStreamer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Converts text chunks to audio and queues for playback                    â”‚
â”‚                                                                            â”‚
â”‚ Features:                                                                 â”‚
â”‚ â€¢ Smart chunking: splits at sentence/phrase boundaries                   â”‚
â”‚ â€¢ Applies ProsodyPlanner before synthesis                                â”‚
â”‚ â€¢ Uses pyttsx3 for local, offline TTS                                    â”‚
â”‚ â€¢ Non-blocking async synthesis                                           â”‚
â”‚ â€¢ Callback-driven playback coordination                                  â”‚
â”‚                                                                            â”‚
â”‚ Returns: AudioChunk objects with metadata (sequence, duration, is_final) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ AudioConversationOrchestrator â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Main coordinator managing full conversation cycle                        â”‚
â”‚                                                                            â”‚
â”‚ State Machine:                                                            â”‚
â”‚ â€¢ IDLE         â†’ Waiting for user input                                  â”‚
â”‚ â€¢ RECORDING    â†’ Capturing audio                                         â”‚
â”‚ â€¢ TRANSCRIBING â†’ Converting speech to text (Whisper)                     â”‚
â”‚ â€¢ PROCESSING   â†’ Running through FirstPerson pipeline                    â”‚
â”‚ â€¢ SPEAKING     â†’ Playing back response audio                             â”‚
â”‚ â€¢ PAUSED       â†’ User paused conversation                                â”‚
â”‚ â€¢ STOPPED      â†’ User stopped or max turns reached                       â”‚
â”‚                                                                            â”‚
â”‚ Control Methods:                                                          â”‚
â”‚ â€¢ run_conversation_loop()  â†’ Main async loop                             â”‚
â”‚ â€¢ pause()  â†’ Pause at next safe point                                    â”‚
â”‚ â€¢ resume() â†’ Resume from pause                                           â”‚
â”‚ â€¢ stop()   â†’ Stop immediately                                            â”‚
â”‚ â€¢ reset()  â†’ Clear history for new conversation                          â”‚
â”‚                                                                            â”‚
â”‚ Callbacks:                                                                â”‚
â”‚ â€¢ register_state_callback(fn) â†’ Called on state changes                  â”‚
â”‚                                                                            â”‚
â”‚ Returns: List of ConversationTurn objects with full transcript           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""

# ============================================================================

# 3. DATA FLOW

# ============================================================================

""" TURN-BY-TURN DATA FLOW:

1. USER SPEAKS
   â”œâ”€ AudioRecorder.record()
   â””â”€ Returns: np.ndarray (16kHz mono audio)

2. TRANSCRIPTION
   â”œâ”€ WhisperModel.transcribe(audio_data)
   â””â”€ Returns: str (user's text)

3. CONTEXT BUILDING
   â”œâ”€ conversation_history: List[ConversationTurn] (previous exchanges)
   â”œâ”€ turn_number: int (1-indexed)
   â”œâ”€ Any custom context (user preference, etc.)
   â””â”€ Returns: Dict[str, Any]

4. FIRSTPERSON PROCESSING
   â”œâ”€ response_processor(user_text, context)
   â”œâ”€ Can return:
   â”‚  â€¢ (response_text, glyph_intent)  â†’ Full response with prosody
   â”‚  â€¢ response_text                   â†’ Plain text (no glyph prosody)
   â””â”€ Returns: Tuple[str, Optional[Dict]] or str

5. PROSODY PLANNING (if glyph_intent provided)
   â”œâ”€ ProsodyPlanner.plan(response_text, glyph_intent)
   â””â”€ Returns: SSML-marked text with prosody tags

6. TEXT CHUNKING
   â”œâ”€ Split at sentence boundaries (smart)
   â”œâ”€ Target ~100 characters per chunk
   â””â”€ Returns: List[str]

7. TTS SYNTHESIS & STREAMING
   â”œâ”€ For each chunk:
   â”‚  â”œâ”€ pyttsx3.synthesize_chunk(chunk)
   â”‚  â”œâ”€ Save to temp WAV file
   â”‚  â”œâ”€ Read audio data: np.ndarray
   â”‚  â””â”€ Create AudioChunk object
â”œâ”€ Callback:_playback_callback(chunk)
   â””â”€ Returns: None (async generator pattern)

8. NON-BLOCKING PLAYBACK
   â”œâ”€ sounddevice.play(audio_data, blocking=False)
   â”œâ”€ Sleep ~90% of chunk duration (overlap buffer)
   â”œâ”€ Next chunk starts synthesis while current plays
   â””â”€ Continue until all chunks played

9. STORE CONVERSATION TURN
   â”œâ”€ ConversationTurn(user_text, system_response, processing_time)
   â””â”€ Added to conversation_history

10. LOOP OR EXIT
    â”œâ”€ If stop_event set â†’ Exit
    â”œâ”€ If pause_event set â†’ Sleep until resumed
    â”œâ”€ If max_turns reached â†’ Exit
    â””â”€ Otherwise â†’ Back to step 1 (USER SPEAKS)
"""

# ============================================================================

# 4. INTEGRATION WITH FIRSTPERSON PIPELINE

# ============================================================================

""" Your response_processor function MUST follow this signature:

def your_response_processor(user_text: str, context: dict) -> Union[str, Tuple[str, dict]]: '''
Args: user_text (str): User's spoken/typed input context (dict): Contains: â€¢ conversation_history
(List[ConversationTurn]) â€¢ turn_number (int) â€¢ Any other context you add

Returns: str: Plain response text OR Tuple[str, dict]: (response_text, glyph_intent) '''

    # Example: Call FirstPerson tier processing
from emotional_os.deploy.modules.response_handler import handle_response_pipeline

response_text, processing_time = handle_response_pipeline(user_text, context)

    # Example: Extract glyph intent from Glyph signals
glyph_intent = { "voltage": "high",        # User expressing high energy "tone": "positive",       #
Sentiment is positive "certainty": "high",      # Confident response "energy": 0.8,            # 80%
intensity "hesitation": False,      # No pauses }

return response_text, glyph_intent

# Then instantiate orchestrator

orchestrator = AudioConversationOrchestrator( response_processor=your_response_processor,
max_turns=50 )

# Run conversation (from Streamlit or elsewhere)

turns = asyncio.run(orchestrator.run_conversation_loop())

# Access results

for turn in turns: print(f"User: {turn.user_text}") print(f"System: {turn.system_response}")
print(f"Time: {turn.processing_time:.2f}s") print(f"Audio Played: {turn.audio_played}") """

# ============================================================================

# 5. USAGE EXAMPLES

# ============================================================================

""" EXAMPLE 1: Standalone Python Script â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import asyncio from src.emotional_os.deploy.modules.audio_conversation_orchestrator import (
AudioConversationOrchestrator ) from emotional_os.deploy.modules.response_handler import
handle_response_pipeline

def my_response_processor(user_text, context): response,_ = handle_response_pipeline(user_text,
context)

    # Extract glyph-based intent (from your Glyph signals)
glyph_intent = { "voltage": "medium", "tone": "neutral", "certainty": "high", "energy": 0.5, }

return response, glyph_intent

orchestrator = AudioConversationOrchestrator(my_response_processor, max_turns=10)

# Add state callback for logging

def on_state_change(state): print(f"[State] {state.value.upper()}")

orchestrator.register_state_callback(on_state_change)

# Run conversation

turns = asyncio.run(orchestrator.run_conversation_loop())

# Print transcript

for i, turn in enumerate(turns, 1): print(f"\\n=== Turn {i} ===") print(f"You: {turn.user_text}")
print(f"FirstPerson: {turn.system_response}") print(f"Processing: {turn.processing_time:.2f}s")

â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€

EXAMPLE 2: Streamlit Integration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import streamlit as st import asyncio from
src.emotional_os.deploy.modules.audio_conversation_orchestrator import (
AudioConversationOrchestrator, ConversationState ) from emotional_os.deploy.modules.response_handler
import handle_response_pipeline

st.title("FirstPerson Audio Conversation")

def response_processor(user_text, context):
    # Your FirstPerson processing here
response, _= handle_response_pipeline(user_text, context)

    # Build glyph intent based on response analysis
glyph_intent = { "voltage": "high" if len(response) > 200 else "medium", "tone": "positive",  # or
extract from sentiment analysis "certainty": "high", "energy": 0.7, "hesitation": False, } return
response, glyph_intent

# Initialize session state

if "orchestrator" not in st.session_state: st.session_state.orchestrator =
AudioConversationOrchestrator( response_processor, max_turns=50 )

orchestrator = st.session_state.orchestrator

# UI Layout

col1, col2, col3 = st.columns(3)

with col1: if st.button("ğŸ¤ Start Audio Conversation"): st.session_state.conversation_running = True

with col2: if st.button("â¸ï¸ Pause"): orchestrator.pause()

with col3: if st.button("â¹ï¸ Stop"): orchestrator.stop()

# State display

state_colors = { "idle": "ğŸŸ¢", "recording": "ğŸ”´", "transcribing": "ğŸ”µ", "processing": "ğŸŸ ", "speaking":
"ğŸŸ¡", "paused": "âšª", "stopped": "âš«", }

col_status = st.empty() col_status.write(f"{state_colors.get(orchestrator.state.value, 'â“')} " +
f"**State**: {orchestrator.state.value.upper()}")

# Run conversation if active

if st.session_state.get("conversation_running", False): with st.spinner("Listening..."): turns =
asyncio.run(orchestrator.run_conversation_loop()) st.session_state.conversation_running = False

# Display history

if orchestrator.conversation_history: st.subheader("Conversation Transcript") for i, turn in
enumerate(orchestrator.conversation_history, 1): with st.expander(f"Turn {i}:
{turn.user_text[:50]}..."): st.write(f"**You:** {turn.user_text}") st.write(f"**FirstPerson:**
{turn.system_response}")
            st.caption(f"â±ï¸ {turn.processing_time:.2f}s | ğŸ”Š Audio played: {turn.audio_played}")

â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€

EXAMPLE 3: Pause/Resume During Conversation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# In a background thread or event handler

import time

def monitor_user_input(): while not stop_event.is_set(): user_input = input("Command (p=pause,
r=resume, s=stop): ")

if user_input.lower() == "p": orchestrator.pause() print("â¸ï¸ Paused") elif user_input.lower() ==
"r": orchestrator.resume() print("â–¶ï¸ Resumed") elif user_input.lower() == "s": orchestrator.stop()
print("â¹ï¸ Stopped")

time.sleep(0.1)

# Run in separate thread

import threading monitor_thread = threading.Thread(target=monitor_user_input, daemon=True)
monitor_thread.start()

turns = asyncio.run(orchestrator.run_conversation_loop()) """

# ============================================================================

# 6. TROUBLESHOOTING

# ============================================================================

""" ISSUE: "sounddevice unavailable: PortAudio library not found"
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Cause: PortAudio development headers
not installed Fix: â€¢ Linux: apt-get install portaudio19-dev â€¢ macOS: brew install portaudio â€¢
Windows: Already in Dockerfile.streamlit

ISSUE: spaCy model not found during warmup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Cause: NLP
initialization before Docker build completes Fix: â€¢ Ensure nlp_init.py runs successfully in
container â€¢ Check Dockerfile installs spaCy: python -m spacy download en_core_web_sm
  â€¢ Verify container logs: docker logs firstperson_streamlit | grep -i spacy

ISSUE: Transcription fails (WhisperModel issues) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Cause: faster-whisper not installed or audio quality poor Fix: â€¢ Ensure faster-whisper in
requirements.txt â€¢ Check audio levels: confirm AudioRecorder recording properly â€¢ Try shorter
recording duration (max_duration param in recorder.record)

ISSUE: TTS chunks not playing seamlessly â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Cause: Blocking
playback or synthesis latency Fix: â€¢ Verify playback is non-blocking: sd.play(..., blocking=False) â€¢
Increase chunk duration (reduce chunk_size in TextToSpeechStreamer) â€¢ Check pyttsx3 engine state
between chunks

ISSUE: High latency between user input and response
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Cause: Processing time, transcription delay, or
synthesis bottleneck Fix: â€¢ Profile response_processor timing â€¢ Use Whisper "tiny" or "base" for
faster transcription â€¢ Pre-warm TTS engine on startup â€¢ Consider chunking earlier (split text while
still processing)

ISSUE: Glyph intent not affecting prosody â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Cause:
response_processor not returning glyph_intent tuple Fix: â€¢ Ensure response_processor returns (text,
glyph_intent) not just text â€¢ Check ProsodyPlanner mapping matches your glyph schema â€¢ Verify
pyttsx3 supports SSML (some platforms don't fully support it) """

# ============================================================================

# END OF GUIDE

# ============================================================================
