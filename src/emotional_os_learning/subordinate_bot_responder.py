"""Subordinate Bot Responder for FirstPerson.

The performer/actor layer: Fast, local response generation using existing glyphs.
NEVER learns, NEVER blocks, NEVER calls expensive APIs.

Pipeline:
  1. Receive user input
  2. Parse emotional context from Tier1/2/3
  3. Find best matching glyph
  4. Generate response from glyph
  5. Return immediately

Target: <100ms response time
"""

from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple
import numpy as np
from scipy.spatial.distance import cosine


@dataclass
class SubordinateBotResponse:
    """Response generated by subordinate bot."""
    response_text: str
    glyph_name: str
    emotional_vector: List[float]
    confidence: float
    processing_time_ms: float
    method: str  # "matched" or "synthesized"


class SubordinateBotResponder:
    """Fast local responder using existing glyphs.
    
    Never creates new glyphs, never learns, never changes.
    It's a pure inference engine.
    """

    def __init__(
        self,
        tier1_foundation,
        tier2_aliveness,
        tier3_poetic_consciousness,
        glyph_library: Optional[Dict] = None,
    ):
        """Initialize subordinate bot.
        
        Args:
            tier1_foundation: Tier1 instance (for safety, wrapping)
            tier2_aliveness: Tier2 instance (for presence, energy)
            tier3_poetic_consciousness: Tier3 instance (for depth, beauty)
            glyph_library: Dictionary of available glyphs {glyph_name: glyph_data}
        """
        self.tier1 = tier1_foundation
        self.tier2 = tier2_aliveness
        self.tier3 = tier3_poetic_consciousness
        self.glyph_library = glyph_library or {}
        self.response_history = []

    def respond(
        self,
        user_input: str,
        conversation_context: Dict,
        emotional_vector: List[float],
    ) -> SubordinateBotResponse:
        """Generate response WITHOUT learning.
        
        Args:
            user_input: The user's message
            conversation_context: Conversation metadata
            emotional_vector: Parsed emotional vector from input
            
        Returns:
            SubordinateBotResponse with text and metadata
        """
        import time
        start_time = time.time()
        
        # Step 1: Find best matching glyph
        best_glyph_name, match_confidence = self._get_best_glyph_match(
            emotional_vector
        )
        
        # Step 2: Generate response from glyph
        if best_glyph_name and match_confidence > 0.3:  # Decent match
            response_text, method = self._generate_response_from_glyph(
                best_glyph_name,
                user_input,
                conversation_context,
            )
        else:  # No good match - use fallback
            response_text, method = self._generate_fallback_response(
                user_input,
                conversation_context,
            )
            best_glyph_name = "fallback"
            match_confidence = 0.0
        
        # Step 3: Apply tier processing (safety, presence, depth)
        response_text = self.tier1.wrap_response(response_text, conversation_context)
        response_text = self.tier2.attune_presence(response_text, conversation_context)
        response_text = self.tier3.enrich_with_poetry(response_text, conversation_context)
        
        # Step 4: Calculate processing time
        processing_time_ms = (time.time() - start_time) * 1000
        
        # Create response object
        response = SubordinateBotResponse(
            response_text=response_text,
            glyph_name=best_glyph_name,
            emotional_vector=emotional_vector,
            confidence=match_confidence,
            processing_time_ms=processing_time_ms,
            method=method,
        )
        
        # Store for dominant bot to observe
        self.response_history.append(
            {
                "user_input": user_input,
                "response": response_text,
                "glyph": best_glyph_name,
                "confidence": match_confidence,
                "emotional_vector": emotional_vector,
            }
        )
        
        return response

    def _get_best_glyph_match(
        self,
        emotional_vector: List[float],
        top_k: int = 3,
    ) -> Tuple[Optional[str], float]:
        """Find best matching glyph using cosine similarity.
        
        Args:
            emotional_vector: Current emotional vector
            top_k: Return top K matches
            
        Returns:
            (best_glyph_name, match_confidence)
        """
        if not self.glyph_library:
            return None, 0.0
        
        query_vector = np.array(emotional_vector)
        similarities = {}
        
        for glyph_name, glyph_data in self.glyph_library.items():
            glyph_vector = np.array(glyph_data.get("emotional_vector", []))
            
            if len(glyph_vector) != len(query_vector):
                continue  # Dimension mismatch
            
            # Cosine similarity: higher is better (range 0-1)
            similarity = 1 - cosine(query_vector, glyph_vector)
            similarities[glyph_name] = similarity
        
        if not similarities:
            return None, 0.0
        
        # Return best match
        best_glyph = max(similarities, key=similarities.get)
        best_confidence = similarities[best_glyph]
        
        return best_glyph, best_confidence

    def _generate_response_from_glyph(
        self,
        glyph_name: str,
        user_input: str,
        conversation_context: Dict,
    ) -> Tuple[str, str]:
        """Generate response using a matched glyph.
        
        Args:
            glyph_name: Name of matched glyph
            user_input: User's message
            conversation_context: Conversation metadata
            
        Returns:
            (response_text, method="matched")
        """
        if glyph_name not in self.glyph_library:
            return self._generate_fallback_response(user_input, conversation_context)
        
        glyph = self.glyph_library[glyph_name]
        
        # Use glyph's response template or examples
        if "response_template" in glyph:
            response_text = glyph["response_template"].format(
                user_input=user_input,
                **conversation_context,
            )
        elif "examples" in glyph and glyph["examples"]:
            # Pick first example as response kernel
            response_text = glyph["examples"][0]
        else:
            # Fallback if glyph has no response data
            return self._generate_fallback_response(user_input, conversation_context)
        
        return response_text, "matched"

    def _generate_fallback_response(
        self,
        user_input: str,
        conversation_context: Dict,
    ) -> Tuple[str, str]:
        """Generate fallback response when no glyph matches.
        
        Args:
            user_input: User's message
            conversation_context: Conversation metadata
            
        Returns:
            (response_text, method="fallback")
        """
        # Generic fallback - acknowledges and invites deeper conversation
        fallback = f"I hear you. Tell me more about {user_input.split()[0].lower() if user_input else 'that'}."
        return fallback, "fallback"

    def update_glyph_library(self, glyph_library: Dict) -> None:
        """Update available glyphs (called when dominant bot creates new ones).
        
        Args:
            glyph_library: New/updated glyph dictionary
        """
        self.glyph_library = glyph_library