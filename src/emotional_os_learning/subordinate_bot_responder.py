"""Subordinate Bot Responder for FirstPerson.

The performer/actor layer: Fast, local response generation using existing glyphs.
NEVER learns, NEVER blocks, NEVER calls expensive APIs.

Pipeline:
  1. Receive user input
  2. Parse emotional context from Tier1/2/3
  3. Find best matching glyph
  4. Generate response from glyph
  5. Return immediately

Target: <100ms response time
"""

from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple
import numpy as np
from scipy.spatial.distance import cosine


@dataclass
class SubordinateBotResponse:
    """Response generated by subordinate bot."""
    response_text: str
    glyph_name: str
    emotional_vector: List[float]
    confidence: float
    processing_time_ms: float
    method: str  # "matched" or "synthesized"


class SubordinateBotResponder:
    """Fast local responder using existing glyphs.
    
    Never creates new glyphs, never learns, never changes.
    It's a pure inference engine.
    """

    def __init__(
        self,
        tier1_foundation,
        tier2_aliveness,
        tier3_poetic_consciousness,
        glyph_library: Optional[Dict] = None,
    ):
        """Initialize subordinate bot.
        
        Args:
            tier1_foundation: Tier1 instance (for safety, wrapping)
            tier2_aliveness: Tier2 instance (for presence, energy)
            tier3_poetic_consciousness: Tier3 instance (for depth, beauty)
            glyph_library: Dictionary of available glyphs {glyph_name: glyph_data}
        """
        self.tier1 = tier1_foundation
        self.tier2 = tier2_aliveness
        self.tier3 = tier3_poetic_consciousness
        self.glyph_library = glyph_library or {}
        self.response_history = []

    def respond(
        self,
        user_input: str,
        conversation_context: Dict,
        emotional_vector: List[float],
    ) -> SubordinateBotResponse:
        """Generate response WITHOUT learning.
        
        Args:
            user_input: The user's message
            conversation_context: Conversation metadata
            emotional_vector: Parsed emotional vector from input
            
        Returns:
            SubordinateBotResponse with text and metadata
        """
        import time
        start_time = time.time()
        
        # Step 1: Find best matching glyph
        best_glyph_name, match_confidence = self._get_best_glyph_match(
            emotional_vector
        )
        
        # Step 2: Generate response from glyph
        if best_glyph_name and match_confidence > 0.3:  # Decent match
            response_text, method = self._generate_response_from_glyph(
                best_glyph_name,
                user_input,
                conversation_context,
            )
        else:  # No good match - use fallback
            response_text, method = self._generate_fallback_response(
                user_input,
                conversation_context,
            )
            best_glyph_name = "fallback"
            match_confidence = 0.0
        
        # Step 3: Apply tier processing (safety, presence)
        # NOTE: Tier3 (poetic enrichment) is intentionally NOT applied here
        # because the subordinate responder must remain fast, grounded, and
        # non-poetic. Tier3 belongs to the dominant/deep layer only.
        response_text = self.tier1.wrap_response(response_text, conversation_context)
        response_text = self.tier2.attune_presence(response_text, conversation_context)
        
        # Step 4: Calculate processing time
        processing_time_ms = (time.time() - start_time) * 1000
        
        # Create response object
        response = SubordinateBotResponse(
            response_text=response_text,
            glyph_name=best_glyph_name,
            emotional_vector=emotional_vector,
            confidence=match_confidence,
            processing_time_ms=processing_time_ms,
            method=method,
        )
        
        # Store for dominant bot to observe
        self.response_history.append(
            {
                "user_input": user_input,
                "response": response_text,
                "glyph": best_glyph_name,
                "confidence": match_confidence,
                "emotional_vector": emotional_vector,
            }
        )
        
        return response

    def _get_best_glyph_match(
        self,
        emotional_vector: List[float],
        top_k: int = 3,
    ) -> Tuple[Optional[str], float]:
        """Find best matching glyph using cosine similarity.
        
        Args:
            emotional_vector: Current emotional vector
            top_k: Return top K matches
            
        Returns:
            (best_glyph_name, match_confidence)
        """
        if not self.glyph_library:
            return None, 0.0
        
        query_vector = np.array(emotional_vector)
        similarities = {}
        
        for glyph_name, glyph_data in self.glyph_library.items():
            glyph_vector = np.array(glyph_data.get("emotional_vector", []))
            
            if len(glyph_vector) != len(query_vector):
                continue  # Dimension mismatch
            
            # Cosine similarity: higher is better (range 0-1)
            similarity = 1 - cosine(query_vector, glyph_vector)
            similarities[glyph_name] = similarity
        
        if not similarities:
            return None, 0.0
        
        # Return best match
        best_glyph = max(similarities, key=similarities.get)
        best_confidence = similarities[best_glyph]
        
        return best_glyph, best_confidence

    def _generate_response_from_glyph(
        self,
        glyph_name: str,
        user_input: str,
        conversation_context: Dict,
    ) -> Tuple[str, str]:
        """Generate response using a matched glyph.
        
        Args:
            glyph_name: Name of matched glyph
            user_input: User's message
            conversation_context: Conversation metadata
            
        Returns:
            (response_text, method="matched")
        """
        # Allow generating a response based on glyph_name even if the glyph
        # isn't present in the library (we use glyph name as emotional direction).
        glyph = self.glyph_library.get(glyph_name, {})

        # Simplified mapping: use glyph name for emotional direction only.
        # Return short, human, grounded responses (no poetry, no Tier3 here).
        name = (glyph_name or "").lower()
        ui = (user_input or "").lower()

        # Direct glyph-based mappings
        if any(k in name for k in ("sad", "grief", "mourning", "ache")):
            return "That sounds rough. What happened?", "matched"

        if any(k in name for k in ("anger", "frustration")):
            return "Yeah, I get why that would hit hard. What set it off?", "matched"

        if any(k in name for k in ("fear", "anxiety", "panic")):
            return "That sounds stressful. What's making it feel that way?", "matched"

        if any(k in name for k in ("joy", "relief", "happy")):
            return "Nice — sounds like something good happened.", "matched"

        # Quick user-cue overrides
        if any(w in ui for w in ("tired", "exhausted", "beat", "sleepy")):
            return "You sound wiped. What's been draining you?", "matched"

        # Default matched response (short and casual)
        return "Okay, I’m with you. Tell me what’s going on.", "matched"

    def _generate_fallback_response(
        self,
        user_input: str,
        conversation_context: Dict,
    ) -> Tuple[str, str]:
        """Generate fallback response when no glyph matches.
        
        Args:
            user_input: User's message
            conversation_context: Conversation metadata
            
        Returns:
            (response_text, method="fallback")
        """
        # Rotate through short, casual fallback patterns for a more human feel
        fallbacks = [
            "Got it. What’s going on?",
            "Okay. Say more.",
            "Alright — walk me through it.",
            "Yeah, I’m here. What happened?",
            "Mm, okay. What part is hitting you the most?",
            "I get you. Tell me more.",
        ]
        import random
        return random.choice(fallbacks), "fallback"

    def update_glyph_library(self, glyph_library: Dict) -> None:
        """Update available glyphs (called when dominant bot creates new ones).
        
        Args:
            glyph_library: New/updated glyph dictionary
        """
        self.glyph_library = glyph_library