"""
Audio Conversation System - Implementation Checklist

Steps to fully integrate audio conversation into FirstPerson Streamlit app.
Check off each item as you complete it.

Created: 2025-12-11
Status: Ready for implementation
"""

# ============================================================================
# PHASE 1: DEPENDENCIES & INFRASTRUCTURE
# ============================================================================

PHASE_1_DEPENDENCIES = """
[‚úì] 1. System Dependencies (Dockerfile already includes):
    ‚Ä¢ portaudio19-dev (for sounddevice audio I/O)
    ‚Ä¢ libsndfile1 (for sound file handling)
    ‚Ä¢ ffmpeg (for audio format conversion)
    ‚Ä¢ gcc (for compilation)

[‚úì] 2. Python Packages (requirements.txt already includes):
    ‚Ä¢ sounddevice>=0.4.5         (audio recording/playback)
    ‚Ä¢ faster-whisper>=0.10.0     (speech-to-text)
    ‚Ä¢ pyttsx3>=2.90              (text-to-speech, local)
    ‚Ä¢ scipy>=1.11.0              (audio processing, silence detection)
    ‚Ä¢ numpy>=1.24.0              (audio data handling)
    ‚Ä¢ asyncio                    (built-in, async orchestration)

[‚úì] 3. Docker Image Rebuild:
    Command: docker compose -f docker-compose.local.yml build --no-cache streamlit
    
    Verification:
    ‚Ä¢ docker exec firstperson_streamlit python -c "import sounddevice; print('‚úì sounddevice')"
    ‚Ä¢ docker exec firstperson_streamlit python -c "import faster_whisper; print('‚úì faster-whisper')"
    ‚Ä¢ docker exec firstperson_streamlit python -c "import pyttsx3; print('‚úì pyttsx3')"
"""

# ============================================================================
# PHASE 2: CORE MODULE SETUP
# ============================================================================

PHASE_2_MODULES = """
[‚úì] 1. prosody_planner.py (CREATED):
    Location: src/emotional_os/deploy/modules/prosody_planner.py
    Purpose: Convert glyph signals to SSML prosody tags
    Key Classes:
    ‚Ä¢ ProsodyPlanner: Maps voltage/tone/certainty to speech characteristics
    
    Test: python -c "from src.emotional_os.deploy.modules.prosody_planner import ProsodyPlanner; p = ProsodyPlanner(); print(p.get_prosody_summary({'voltage': 'high'}))"

[‚úì] 2. audio_conversation_orchestrator.py (UPDATED):
    Location: src/emotional_os/deploy/modules/audio_conversation_orchestrator.py
    Improvements Made:
    ‚Ä¢ Non-blocking playback with overlap buffer
    ‚Ä¢ Glyph intent support in TTS streaming
    ‚Ä¢ ProsodyPlanner integration
    ‚Ä¢ Response processor now returns (text, glyph_intent) tuples
    ‚Ä¢ 250ms initial playback buffer for smoother start
    
    Key Classes:
    ‚Ä¢ AudioRecorder: Captures speech with silence detection
    ‚Ä¢ TextToSpeechStreamer: Chunks text & synthesizes audio
    ‚Ä¢ AudioConversationOrchestrator: Main coordination loop
    ‚Ä¢ ProsodyPlanner: (imported) Prosody control

[  ] 3. UI Integration (NEXT):
    Location: src/emotional_os/deploy/modules/ui_refactored.py (or new module)
    Task: Add audio conversation UI to Streamlit
    
    What to Add:
    ‚Ä¢ "üé§ Start Audio Conversation" button
    ‚Ä¢ State indicator (live status display)
    ‚Ä¢ Pause/Resume/Stop controls
    ‚Ä¢ Conversation transcript display (expandable turns)
    ‚Ä¢ Real-time state callbacks to update UI
    
    Example Integration:
    from audio_conversation_orchestrator import AudioConversationOrchestrator
    from prosody_planner import ProsodyPlanner
    
    orchestrator = AudioConversationOrchestrator(
        response_processor=your_response_processor,
        max_turns=50
    )
    
    orchestrator.register_state_callback(st.session_state["update_state"])
    
    if st.button("üé§ Start"):
        turns = asyncio.run(orchestrator.run_conversation_loop())
"""

# ============================================================================
# PHASE 3: FIRSTPERSON INTEGRATION
# ============================================================================

PHASE_3_INTEGRATION = """
[  ] 1. Update response_processor signature:
    Current: handle_response_pipeline(user_text, context) ‚Üí (text, time)
    
    New wrapper needed:
    def firstperson_audio_response_processor(user_text, context):
        response, processing_time = handle_response_pipeline(user_text, context)
        
        # Extract glyph-based intent (CRITICAL)
        glyph_intent = extract_glyph_intent(response, context)
        
        return response, glyph_intent
    
    Where glyph_intent dict contains:
    {
        "voltage": "low" | "medium" | "high",
        "tone": "negative" | "neutral" | "positive",
        "certainty": "low" | "neutral" | "high",
        "energy": 0.0-1.0 (float),
        "hesitation": bool,
        "phoneme_stretch": 1.0 (float, 1.0=normal)
    }

[  ] 2. Extract glyph signals for prosody:
    Suggested locations to extract signals:
    
    ‚Ä¢ Tier 1 Foundation (base emotional state):
      - valence (negative-positive) ‚Üí tone mapping
      - arousal (low-high) ‚Üí voltage mapping
    
    ‚Ä¢ Tier 2 Aliveness (presence intensity):
      - energy level ‚Üí energy parameter
      - presence strength ‚Üí volume scaling
    
    ‚Ä¢ Tier 3 Poetic Consciousness (complexity):
      - certainty metric ‚Üí certainty parameter
      - introspection depth ‚Üí hesitation/pauses
    
    Example extraction:
    def extract_glyph_intent(response_text, context):
        # Get latest glyph signals from Tier 2/3
        glyph_state = get_glyph_state()  # Your method
        
        return {
            "voltage": map_arousal_to_voltage(glyph_state.arousal),
            "tone": map_valence_to_tone(glyph_state.valence),
            "certainty": map_confidence_to_certainty(glyph_state.confidence),
            "energy": glyph_state.energy_level,
            "hesitation": glyph_state.introspection_depth > 0.7,
            "phoneme_stretch": 1.0,
        }

[  ] 3. Test integrated pipeline:
    Step-by-step test:
    
    a) Start Streamlit app:
       streamlit run app.py
    
    b) Navigate to audio conversation section
    
    c) Click "üé§ Start Audio Conversation"
    
    d) Speak test input: "How are you?"
    
    e) Verify:
       ‚úì Audio recorded (waveform captured)
       ‚úì Transcribed correctly (text appears)
       ‚úì Response generated (FirstPerson response appears)
       ‚úì Glyph intent extracted (logged)
       ‚úì Prosody applied (speech sounds natural)
       ‚úì Audio plays back (hear system response)
       ‚úì State updates (UI shows: RECORDING ‚Üí TRANSCRIBING ‚Üí PROCESSING ‚Üí SPEAKING ‚Üí IDLE)
    
    f) Check logs:
       docker logs firstperson_streamlit | grep -i "audio\|prosody\|glyph"
"""

# ============================================================================
# PHASE 4: OPTIMIZATION & TUNING
# ============================================================================

PHASE_4_OPTIMIZATION = """
[  ] 1. Latency Profiling:
    Measure where time is spent:
    
    ‚Ä¢ Recording latency:
      - Start recording
      - Check time to first audio frame
      - Should be <100ms
    
    ‚Ä¢ Transcription latency:
      - Measure Whisper model inference time
      - Tune: use "tiny" for speed, "base" for accuracy
    
    ‚Ä¢ Processing latency:
      - Profile FirstPerson response pipeline
      - Check Tier 1/2/3 execution times
    
    ‚Ä¢ TTS latency:
      - Measure pyttsx3 synthesis time per chunk
      - Goal: synthesis faster than playback (9/10 of chunk duration)
    
    ‚Ä¢ Playback latency:
      - Measure time from playback start to first audio output
      - Should be <50ms (sounddevice overhead)
    
    Total Target: <2 seconds from speech end to response audio start

[  ] 2. Prosody Tuning:
    Fine-tune ProsodyPlanner mappings:
    
    ‚Ä¢ Voltage calibration:
      - Test different "rate" values (slow/medium/fast)
      - Match to arousal levels in your domain
    
    ‚Ä¢ Tone calibration:
      - Test different "pitch" values (low/medium/high)
      - Ensure negative‚Üílow, positive‚Üíhigh makes sense
    
    ‚Ä¢ Certainty calibration:
      - Test intonation contours (rising/neutral/falling)
      - Verify falling sounds confident, rising sounds questioning
    
    ‚Ä¢ Energy modulation:
      - Adjust volume scaling based on energy value
      - Test 0.0-1.0 range with different response texts
    
    Create test cases:
    glyph_intent_confident = {"voltage": "high", "tone": "positive", "certainty": "high"}
    glyph_intent_uncertain = {"voltage": "low", "tone": "negative", "certainty": "low"}
    
    Test both with same text to hear prosody differences

[  ] 3. Audio Quality Tuning:
    ‚Ä¢ Silence detection:
      - Current: 1.5 seconds of <0.02 RMS amplitude
      - Tune silence_threshold if too sensitive/insensitive
      - Tune silence_duration if recordings cut off too early/late
    
    ‚Ä¢ Sample rate:
      - Current: 16kHz (good balance of quality/size)
      - Increase to 48kHz for higher quality if CPU allows
    
    ‚Ä¢ Chunk size:
      - Current: ~100 characters per chunk
      - Increase for longer phrases (less switching)
      - Decrease for punchier delivery
    
    ‚Ä¢ Playback buffer:
      - Current: 0.25 seconds (250ms) before playback
      - Increase if synthesis can't keep up
      - Decrease if latency is priority

[  ] 4. Error Handling & Fallbacks:
    Test failure scenarios:
    
    ‚Ä¢ No audio input:
      - Microphone disconnected
      - Proper error message to user
    
    ‚Ä¢ Transcription failure:
      - Audio too noisy
      - Offer to re-record or type instead
    
    ‚Ä¢ TTS synthesis failure:
      - pyttsx3 engine crash
      - Fall back to text-only response
    
    ‚Ä¢ Processing timeout:
      - FirstPerson takes >30 seconds
      - Show "Still thinking..." message
      - Allow user to cancel and continue typing

[  ] 5. Performance Monitoring:
    Add to logs:
    
    ‚Ä¢ Per-turn metrics:
      - User speech duration (seconds)
      - Transcription confidence (0-1)
      - Processing time (seconds)
      - Total response duration (seconds)
      - Turn completion time (end-to-end)
    
    ‚Ä¢ Session metrics:
      - Total turns completed
      - Average processing time per turn
      - Error rate (failed transcriptions, etc.)
      - Total session duration
    
    Example logging:
    logger.info(f"Turn {turn_num}: "
               f"speech={recording_time:.1f}s, "
               f"transcribe={transcribe_time:.1f}s, "
               f"process={process_time:.1f}s, "
               f"tts={tts_time:.1f}s, "
               f"total={total_time:.1f}s")
"""

# ============================================================================
# PHASE 5: USER EXPERIENCE
# ============================================================================

PHASE_5_UX = """
[  ] 1. Visual Feedback:
    Create clear UI indicators:
    
    ‚Ä¢ State indicator (animated):
      - IDLE: "üü¢ Ready to listen"
      - RECORDING: "üî¥ Listening..." (with waveform animation)
      - TRANSCRIBING: "üîµ Processing speech..."
      - PROCESSING: "üü† FirstPerson thinking..."
      - SPEAKING: "üü° Speaking response..." (with audio waveform)
      - PAUSED: "‚ö™ Paused"
      - STOPPED: "‚ö´ Stopped"
    
    ‚Ä¢ Real-time waveform visualization:
      - Show incoming audio during recording
      - Show playback waveform during response
    
    ‚Ä¢ Transcript display:
      - Show user's transcription immediately
      - Show system response as it's generated/played

[  ] 2. Control Buttons:
    Place clearly above/below waveform:
    
    ‚Ä¢ üé§ Start Audio Conversation (primary)
    ‚Ä¢ ‚è∏Ô∏è Pause (during SPEAKING)
    ‚Ä¢ ‚ñ∂Ô∏è Resume (during PAUSED)
    ‚Ä¢ ‚èπÔ∏è Stop (anytime)
    ‚Ä¢ üìù Switch to Text (stop audio, fallback to typing)

[  ] 3. Accessibility:
    ‚Ä¢ Add transcription display (for deaf users)
    ‚Ä¢ Volume control for playback
    ‚Ä¢ Speed control for playback (1.0x / 0.8x / 1.2x)
    ‚Ä¢ Haptic feedback option (if device supports)
    ‚Ä¢ Keyboard shortcuts (Space=start, P=pause, S=stop)

[  ] 4. Settings/Configuration:
    Allow users to customize:
    
    ‚Ä¢ Audio sensitivity (silence threshold)
    ‚Ä¢ Maximum recording duration (default 30s)
    ‚Ä¢ Response voice (pitch, rate)
    ‚Ä¢ Prosody intensity (how much glyph signals affect speech)
    ‚Ä¢ Transcript visibility (show/hide)
    ‚Ä¢ Auto-play next turn (loop until user stops)
"""

# ============================================================================
# PHASE 6: DEPLOYMENT & MONITORING
# ============================================================================

PHASE_6_DEPLOYMENT = """
[‚úì] 1. Docker Deployment (Already configured):
    ‚Ä¢ Dockerfile.streamlit includes all audio dependencies
    ‚Ä¢ docker-compose.local.yml configured with Ollama
    ‚Ä¢ Both services running and healthy
    
    Verify:
    docker compose -f docker-compose.local.yml ps

[  ] 2. Environment Configuration:
    Set in docker-compose.local.yml or .env:
    
    ‚Ä¢ WHISPER_MODEL_SIZE: "tiny" | "base" | "small"
                         (tiny=fastest, small=best quality)
    ‚Ä¢ TTS_ENGINE: "pyttsx3" (or future: "elevenlabs", "google", etc.)
    ‚Ä¢ AUDIO_SAMPLE_RATE: 16000 (Hz)
    ‚Ä¢ AUDIO_SILENCE_THRESHOLD: 0.02 (RMS amplitude)
    ‚Ä¢ AUDIO_SILENCE_DURATION: 1.5 (seconds)
    ‚Ä¢ MAX_RECORDING_DURATION: 30 (seconds)
    ‚Ä¢ MAX_CONVERSATION_TURNS: 50 (per session)

[  ] 3. Production Checklist:
    Before deploying to production:
    
    ‚Ä¢ [ ] Load testing: 10+ concurrent audio conversations
    ‚Ä¢ [ ] Stress testing: 100+ turns in one session
    ‚Ä¢ [ ] Error recovery: all failure modes handled gracefully
    ‚Ä¢ [ ] Security: no audio data logged/stored without consent
    ‚Ä¢ [ ] Privacy: GDPR compliance for audio data
    ‚Ä¢ [ ] Licensing: verify pyttsx3 license compatible
    ‚Ä¢ [ ] Performance: <2s latency for 95th percentile
    ‚Ä¢ [ ] Monitoring: all metrics logged and dashboarded

[  ] 4. Scaling Considerations:
    For high-concurrency deployment:
    
    ‚Ä¢ Whisper model: Consider TensorRT optimization
    ‚Ä¢ TTS: Consider cloud TTS (Google, Azure) for parallelism
    ‚Ä¢ Audio storage: Use object storage (S3, etc.) if archiving
    ‚Ä¢ Load balancing: Distribute across multiple Streamlit instances
    ‚Ä¢ Database: Store conversations persistently for analytics
"""

# ============================================================================
# TESTING CHECKLIST
# ============================================================================

TESTING_CHECKLIST = """
[  ] Unit Tests:
    ‚Ä¢ AudioRecorder: silence detection, max duration
    ‚Ä¢ TextToSpeechStreamer: chunking logic, prosody application
    ‚Ä¢ ProsodyPlanner: all mappings, SSML generation
    ‚Ä¢ AudioConversationOrchestrator: state transitions, callbacks

[  ] Integration Tests:
    ‚Ä¢ End-to-end audio conversation (mock FirstPerson response)
    ‚Ä¢ Pause/resume functionality
    ‚Ä¢ Stop during playback
    ‚Ä¢ Multiple consecutive turns
    ‚Ä¢ Error handling (no audio, bad transcription, synthesis failure)

[  ] UI Tests (Manual):
    ‚Ä¢ Button responsiveness
    ‚Ä¢ State display accuracy
    ‚Ä¢ Transcript appearance
    ‚Ä¢ Audio playback (verify sound)
    ‚Ä¢ Mobile responsiveness (if deployed to mobile)

[  ] Performance Tests:
    ‚Ä¢ Latency profiling (record each stage)
    ‚Ä¢ Memory usage during long sessions
    ‚Ä¢ CPU usage during playback
    ‚Ä¢ GPU utilization (if using GPU-accelerated Whisper)
"""

# ============================================================================
# QUICK START GUIDE (For Next Session)
# ============================================================================

QUICK_START = """
To resume work on audio integration:

1. Start Docker:
   docker compose -f docker-compose.local.yml up -d

2. Start Streamlit:
   streamlit run app.py

3. Test audio components:
   python -c "from src.emotional_os.deploy.modules.audio_conversation_orchestrator import AudioConversationOrchestrator; print('‚úì Import successful')"

4. Check logs:
   docker logs firstperson_streamlit | tail -50

5. Access Streamlit:
   Open http://localhost:8501 in browser

6. Begin PHASE 3 (UI Integration) from checklist above

7. For debugging:
   docker exec -it firstperson_streamlit bash
   cd /app && python -m pytest tests/ -v
"""

# ============================================================================
# CURRENT STATUS: 2025-12-11
# ============================================================================

STATUS = """
‚úì COMPLETED:
  - ProsodyPlanner class (prosody_planner.py)
  - AudioRecorder with silence detection
  - TextToSpeechStreamer with chunk queuing
  - AudioConversationOrchestrator with state machine
  - Non-blocking playback integration
  - Glyph intent support throughout pipeline
  - Docker dependencies (PortAudio, ffmpeg, etc.)
  - Documentation (this file + AUDIO_CONVERSATION_INTEGRATION_GUIDE.md)

‚è≥ NEXT (PRIORITY ORDER):
  1. Update response_processor to extract glyph intent
  2. Integrate audio UI into ui_refactored.py
  3. Test full end-to-end pipeline
  4. Prosody tuning based on FirstPerson glyph signals
  5. Performance optimization & latency reduction
  6. Error handling & fallback paths
  7. Production monitoring & deployment

üéØ GOAL:
  Users can have natural, prosodically-expressive conversations with FirstPerson
  using spoken audio, with glyph signals driving speech characteristics in real time.
"""
